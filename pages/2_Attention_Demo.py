import streamlit as st
import numpy as np
import plotly.graph_objects as go
from business_logic.attention_model import AttentionDemo

# Streamlit-Styling verstecken
hide_streamlit_style = """
<style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .stDeployButton {display:none;}
    div[data-testid="stStatusWidget"] {display:none;}

    /* Sidebar breiter machen */
    [data-testid="stSidebar"] {
        min-width: 300px;
        max-width: 300px;
    }
</style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# Seiten-Konfiguration
st.set_page_config(
    page_title="Self-Attention Demo - Pronomen-Resolution",
    page_icon="ðŸ”",
    layout="wide"
)

# Header
col1, col2 = st.columns([1, 8])
with col1:
    st.image("Logo-Design fÃ¼r biaSense.png", width=100)
with col2:
    st.title("Self-Attention Demo: Pronomen-Resolution")

st.markdown("---")

# EinfÃ¼hrung
st.markdown("""
Diese Demo zeigt den grundlegenden Self-Attention-Mechanismus, wie er in Transformer-Modellen verwendet wird.
Das Modell lernt, Pronomen mit ihren BezugswÃ¶rtern zu verknÃ¼pfen.

**Hinweis**: Dies ist ein didaktisches Beispiel mit vortrainierten SÃ¤tzen, kein produktionsfÃ¤higes System.
""")

# Sidebar
with st.sidebar:
    st.header("Trainingsparameter")
    epochs = st.slider("Epochen", 100, 2000, 800, 100)
    learning_rate = st.slider("Lernrate", 0.01, 0.1, 0.05, 0.01)

    st.markdown("---")
    st.header("Ãœber das Modell")
    st.markdown("""
    **Architektur:**
    - Embedding-Dimension: 32
    - Query/Key-Dimension: 32
    - Causal Masking: Aktiviert

    **Trainingsdaten:**
    - 3 BeispielsÃ¤tze
    - Supervised Learning
    """)

# Modell initialisieren und trainieren
if 'model' not in st.session_state or st.sidebar.button("Neu trainieren"):
    with st.spinner("Modell wird trainiert..."):
        model = AttentionDemo()
        losses = model.train(epochs=epochs, lr=learning_rate)
        st.session_state.model = model
        st.session_state.losses = losses
    st.success(f"Training abgeschlossen nach {epochs} Epochen")

model = st.session_state.model

# Trainingsverlauf
st.header("Trainingsverlauf")
if 'losses' in st.session_state:
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        y=st.session_state.losses,
        mode='lines',
        name='Loss',
        line=dict(color='#FF6B6B', width=2)
    ))
    fig.update_layout(
        title="Loss Ã¼ber Epochen",
        xaxis_title="Epoche",
        yaxis_title="Loss",
        hovermode='x',
        height=300
    )
    st.plotly_chart(fig, use_container_width=True)

st.markdown("---")

# Analyse der SÃ¤tze
st.header("Pronomen-Attention Analyse")

# Tabs fÃ¼r die drei SÃ¤tze
tab1, tab2, tab3 = st.tabs([
    f"Satz 1: {model.sentences[0][:30]}...",
    f"Satz 2: {model.sentences[1][:30]}...",
    f"Satz 3: {model.sentences[2][:50]}..."
])


def display_sentence_analysis(sentence_idx, tab):
    with tab:
        # Satz anzeigen
        st.markdown(f"**VollstÃ¤ndiger Satz:**")
        st.info(model.sentences[sentence_idx])

        # Analyse durchfÃ¼hren
        analysis = model.analyze_pronoun(sentence_idx)

        # Pronomen und Ziel hervorheben
        col1, col2 = st.columns(2)
        with col1:
            st.metric(
                "Pronomen",
                f"{analysis['pronoun']} (Position {analysis['pronoun_idx']})"
            )
        with col2:
            st.metric(
                "Ziel-Substantiv",
                f"{analysis['target']} (Position {analysis['target_idx']})"
            )

        st.markdown("---")

        # Attention-Gewichte als Heatmap
        st.subheader("Attention-Gewichte Visualisierung")

        A = model.get_attention_weights(sentence_idx)
        tokens = analysis['tokens']

        # Heatmap erstellen
        fig = go.Figure(data=go.Heatmap(
            z=A,
            x=tokens,
            y=tokens,
            colorscale='RdYlGn',
            text=np.round(A, 3),
            texttemplate='%{text}',
            textfont={"size": 10},
            hoverongaps=False
        ))

        fig.update_layout(
            title=f"Attention Matrix fÃ¼r Satz {sentence_idx + 1}",
            xaxis_title="Attention auf Token",
            yaxis_title="Von Token",
            height=500
        )

        st.plotly_chart(fig, use_container_width=True)

        st.markdown("---")

        # Top-5 Attention-Ziele
        st.subheader(f"Top 5 Attention-Ziele fÃ¼r '{analysis['pronoun']}'")

        for i, target in enumerate(analysis['top_targets']):
            col1, col2, col3 = st.columns([2, 2, 6])

            with col1:
                st.write(f"**Rang {i + 1}**")

            with col2:
                weight_percent = target['weight'] * 100
                st.write(f"{weight_percent:.1f}%")

            with col3:
                if target['is_target']:
                    st.success(f"âœ“ **{target['word']}** (Position {target['index']}) - Korrektes Ziel")
                else:
                    st.write(f"{target['word']} (Position {target['index']})")

        st.markdown("---")

        # Interpretation
        st.subheader("Interpretation")

        correct_weight = analysis['attention_weights'][analysis['target_idx']] * 100

        if correct_weight > 50:
            interpretation = f"Das Modell hat gelernt, dass '{analysis['pronoun']}' hauptsÃ¤chlich auf '{analysis['target']}' achten sollte ({correct_weight:.1f}% Attention-Gewicht)."
            st.success(interpretation)
        elif correct_weight > 20:
            interpretation = f"Das Modell zeigt moderate Attention von '{analysis['pronoun']}' auf '{analysis['target']}' ({correct_weight:.1f}%). Mehr Training kÃ¶nnte die Genauigkeit verbessern."
            st.warning(interpretation)
        else:
            interpretation = f"Das Modell hat noch nicht stark gelernt, '{analysis['pronoun']}' mit '{analysis['target']}' zu verknÃ¼pfen ({correct_weight:.1f}%). LÃ¤ngeres Training empfohlen."
            st.error(interpretation)


# Tabs mit Analyse fÃ¼llen
display_sentence_analysis(0, tab1)
display_sentence_analysis(1, tab2)
display_sentence_analysis(2, tab3)

st.markdown("---")

# Technische Details
with st.expander("Technische Details & EinschrÃ¤nkungen"):
    st.markdown("""
    ### Wie funktioniert Self-Attention?

    1. **Query & Key Projektion**: Jedes Wort wird in Query- und Key-Vektoren transformiert
    2. **Attention Scores**: Berechnung der Ã„hnlichkeit zwischen Queries und Keys
    3. **Causal Masking**: Verhindert Zugriff auf zukÃ¼nftige Tokens
    4. **Softmax**: Normalisierung zu Wahrscheinlichkeitsverteilung

    ### Trainierte Paare
    - "She" â†’ "Anna"
    - "He" â†’ "John"
    - "it" â†’ "animal"

    ### EinschrÃ¤nkungen
    - Funktioniert nur auf den drei Trainingsbeispielen
    - Keine Generalisierung auf neue SÃ¤tze
    - Vereinfachte Architektur ohne Value-Projektionen
    - Keine Multi-Head-Attention
    - Kein Feedforward-Netzwerk
    - Rein didaktisches Beispiel

    ### Formel
    ```
    Attention(Q,K) = softmax(QÂ·K^T / âˆšd_k)
    ```

    Dabei ist d_k die Dimension der Key-Vektoren (hier: 32).
    """)

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: gray;'>
    <p>Self-Attention Demo | Educational Purpose Only | Built with Streamlit</p>
</div>
""", unsafe_allow_html=True)